<!-- GPT-USAGE-HEADER:v1
Type: reference documentation (not executable code).
Rules: Treat as docs; do not run as code. Obey the action schemas in ./01-unified-instruction-set.md.
-->
Alpha – Reinforcement Learning Integration & Continuous Improvement

Alpha leverages a dedicated reinforcement learning backend (the FinRL-Actions service) to continuously improve its strategy and adapt to new market data. This integration allows Alpha to learn from each trade and to update its trading parameters for optimal performance. Key aspects of this integration include journaling trade data, training the model, and applying the model’s output for signal refinement:

Structured Trade Journaling: Every trade that Alpha assists with is logged in a structured journal entry. Each journal entry records details such as the symbol traded, the strategy “card” or setup type used, entry and exit timestamps, entry and exit prices, position size, the outcome of the trade (win or loss), the profit/loss or R-multiple (return relative to risk) achieved, and any notes on adjustments during the trade. Alpha also logs detected signals (even if not acted upon) and any aborted trades or errors (e.g., order rejections) in this journal. This comprehensive logging ensures that we have high-quality historical data capturing what the strategy did in various scenarios. The journal serves two purposes: it provides transparency and reviewability of Alpha’s actions, and it forms the dataset for training the reinforcement learning model to refine strategy parameters.

FinRL Training Pipeline: Alpha periodically uses the accumulated journal data to train the FinRL reinforcement learning model. This can be initiated on-demand by the user or on a scheduled basis (for example, after a set number of trades or time interval). When training is triggered (via the FinRL service’s /train endpoint), Alpha supplies the journal entries as input. The FinRL backend processes this data – effectively learning from the historical decisions and outcomes. The training process yields optimized strategy parameters encapsulated in a new version of the model. FinRL’s training focuses on adjusting the strategy’s risk-reward parameters and signal quality based on actual performance. For instance, it evaluates how often trades won vs. lost (win rate) for each type of setup and each symbol, how large the losses were relative to initial risk (to gauge if stop-losses were too tight or too loose), and how large the wins were (to adjust profit targets). By analyzing these, the RL model can tune parameters such as recommended stop-loss distance or take-profit levels to improve future performance. Alpha ensures that each training run is stored with a unique version identifier (timestamped) so results can be tracked over time.

Interpreting Tuned Parameters: After training, the FinRL service produces a set of tuned parameters and stores them accessible via the /predict endpoint. Alpha retrieves the latest parameters (either automatically after training or on demand) to update its strategy settings. These parameters often include metrics like:

Stop-Loss Multiplier (stop_multiplier): A factor suggesting how far the stop-loss should typically be placed relative to recent volatility or risk (e.g., derived from the median adverse price movement observed in losing trades). Alpha will use this to calibrate default stop distances for each strategy or symbol (ensuring stops are not too tight to avoid premature stop-outs, yet not too loose to limit risk).

Take-Profit Multiplier (tp_multiplier): A factor for setting profit targets (e.g., default could be 2.0× risk, adjusted up to 2.5× if a particular strategy and symbol combo has a high historical win rate). This helps Alpha know when to aim for a larger profit vs. secure a moderate one, based on what has been effective historically.

Win Rate and other stats: The model provides the observed win percentage for a given strategy scenario, which Alpha can use as a confidence indicator. A high win rate might justify slightly more aggressive targets, whereas a lower win rate suggests staying conservative with profit expectations or tightening stops.

Alpha integrates these outputs by updating its internal decision rules. For example, if FinRL indicates that trades on Symbol X using Strategy Y have an excellent win rate and minimal drawdowns, Alpha might adopt the higher take-profit multiplier and keep using moderate stops for that case. Conversely, if another strategy shows frequent small losses, Alpha might tighten that strategy’s stop criteria or reduce position size when those signals appear. The end result is that Alpha’s strategy evolves based on real performance data, maintaining or improving its edge.

Using FinRL Signals (Optional): In addition to tuning risk parameters, the reinforcement learning model can also generate direct trading signals. When configured for autonomous signals (SIGNALS_BACKEND="finrl"), Alpha will query the FinRL service’s predictions to identify if the model is suggesting any new trades (for instance, the model might output an action like “enter long on XYZ” or “close position on ABC”). By default, Alpha operates in baseline mode (no automated entry signals, only manual or user-initiated trades). However, if the user enables FinRL signal mode, Alpha’s Protection Daemon will fetch these model-driven trade suggestions each cycle. Importantly, even in this mode, Alpha exercises caution: it runs in “dry run” by default (SIGNALS_DRY_RUN=true), meaning it will log or communicate any model-recommended trades without automatically executing them. This allows the user to observe and evaluate the AI’s trade ideas in real time. Only after the user explicitly decides and turns off dry-run (SIGNALS_DRY_RUN=false), will Alpha start to execute model-proposed trades automatically. This staged approach ensures that automated trading is only activated after careful evaluation and confidence in the model’s performance. Alpha’s design thereby supports a spectrum from user-guided operation to fully autonomous trading, but always gated by user control and thorough testing.

Continuous Improvement Cycle: The integration of journaling, training, and applying new parameters creates a feedback loop for continuous improvement. Alpha effectively gets smarter and more tuned with experience – learning what works and what doesn’t. For example, if the journal shows that a certain pattern yields poor results, the RL model will adjust, and Alpha will stop pursuing trades with that pattern or will handle them differently. Conversely, if a setup performs exceptionally well, Alpha may place more emphasis on it (within risk limits). This adaptive behavior is grounded in quantitative training rather than subjective judgment. Alpha will communicate any significant strategy updates to the user (for instance, “The model has adjusted optimal stop distance for tech stocks based on the last training cycle”). Nonetheless, Alpha will not abruptly change its core strategy without evidence; all adjustments are gradual and data-driven to avoid strategy drift. This gives the user confidence that Alpha’s evolution is making the strategy more robust, not introducing unchecked risks.

Overall, Alpha’s FinRL integration means the assistant is not static – it learns from every trade. By combining human oversight with machine learning, Alpha aims to deliver ever-improving trade decisions while upholding the consistent rules and safety checks of the strategy.